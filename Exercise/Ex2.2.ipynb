{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "                self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "                self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "                self.fc1 = nn.Linear(3*3*64, 256)\n",
    "                self.fc2 = nn.Linear(256, 10)\n",
    "                \n",
    "        def forward(self, x):\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "                x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "                x = x.view(-1,3*3*64 )\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.dropout(x, training=self.training)\n",
    "                x = self.fc2(x)\n",
    "                return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_model_loss = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "      \n",
    "            prediction = model.forward(data)\n",
    "\n",
    "            train_loss = criterion(prediction, labels)\n",
    "\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "        print(f'\\rEpoch {epoch+1}, batch {i+1}/{len(train_loader)} - Loss: {train_loss}')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        writer.add_scalar(\"Loss/train ADAM\", train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "            prediction = model.forward(data)\n",
    "            loss_val = criterion(prediction, labels)\n",
    "            valid_losses.append(loss_val)\n",
    "        print(f\"loss validation: {loss_val}\")\n",
    "\n",
    "        if valid_losses[-1] < best_model_loss:\n",
    "            print(f\"\\t > Found a better model, {best_model_loss} -> {valid_losses[-1]}\")\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_loss = valid_losses[-1]\n",
    "\n",
    "        writer.add_scalar(\"Loss/validation ADAM\", loss_val, epoch)\n",
    "\n",
    "    print(f\"\\nBest model loss: {best_model_loss}\")\n",
    "    return best_model, train_losses, valid_losses\n",
    "\n",
    "def get_accuracy(network, loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        for x, (data, labels) in enumerate(loader):\n",
    "\n",
    "            prediction = network.forward(data)\n",
    "\n",
    "            for i in range(len(data)):\n",
    "\n",
    "                y_true.append(labels[i].item())\n",
    "                y_pred.append(torch.argmax(prediction[i]).item())\n",
    "                if y_true[i] == y_pred[i]:\n",
    "                    correct += 1        \n",
    "    \n",
    "            total += float(len(data))\n",
    "    \n",
    "        score = correct/total\n",
    "\n",
    "        accuracy = score\n",
    "\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 50/50 - Loss: 0.32739126682281494\n",
      "loss validation: 0.37020277976989746\n",
      "\t > Found a better model, 5 -> 0.37020277976989746\n",
      "Epoch 2, batch 50/50 - Loss: 0.20867928862571716\n",
      "loss validation: 0.21903079748153687\n",
      "\t > Found a better model, 0.37020277976989746 -> 0.21903079748153687\n",
      "Epoch 3, batch 50/50 - Loss: 0.1739194095134735\n",
      "loss validation: 0.179385244846344\n",
      "\t > Found a better model, 0.21903079748153687 -> 0.179385244846344\n",
      "Epoch 4, batch 50/50 - Loss: 0.10918772965669632\n",
      "loss validation: 0.1228712722659111\n",
      "\t > Found a better model, 0.179385244846344 -> 0.1228712722659111\n",
      "Epoch 5, batch 50/50 - Loss: 0.09994558990001678\n",
      "loss validation: 0.09697303920984268\n",
      "\t > Found a better model, 0.1228712722659111 -> 0.09697303920984268\n",
      "\n",
      "Best model loss: 0.09697303920984268\n",
      "Model Accuracy (MNIST): 95.7%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,download=True, transform=transform)\n",
    "\n",
    "validset, trainset = torch.utils.data.random_split(trainset, [10000, 50000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model, train_loss, valid_loss = train_model(model, criterion, optimizer, trainloader, validloader, EPOCHS)\n",
    "\n",
    "# Test the model\n",
    "test_acc = get_accuracy(trained_model, testloader)\n",
    "print(f\"Model Accuracy (MNIST): {test_acc*100}%\")\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Model Accuracy (SVHN): 22.6%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),transforms.Grayscale(num_output_channels=1),transforms.Resize(28)])\n",
    "\n",
    "dataset = torchvision.datasets.SVHN(root='./data',download=True,transform=transform)\n",
    "\n",
    "testset, validset, trainset = torch.utils.data.random_split(dataset, [10000,12000,51257])\n",
    "\n",
    "trainloader_svhn = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "validloader_svhn = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader_svhn = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "test_acc = get_accuracy(trained_model, testloader_svhn)\n",
    "print(f\"Model Accuracy (SVHN): {test_acc*100}%\")\n",
    "writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 52/52 - Loss: 1.1429128646850586\n",
      "loss validation: 1.2087353467941284\n",
      "\t > Found a better model, 5 -> 1.2087353467941284\n",
      "Epoch 2, batch 52/52 - Loss: 1.2255258560180664\n",
      "loss validation: 1.1519882678985596\n",
      "\t > Found a better model, 1.2087353467941284 -> 1.1519882678985596\n",
      "Epoch 3, batch 52/52 - Loss: 1.165192723274231\n",
      "loss validation: 1.1611745357513428\n",
      "Epoch 4, batch 52/52 - Loss: 1.0515843629837036\n",
      "loss validation: 1.178928017616272\n",
      "Epoch 5, batch 52/52 - Loss: 1.098952293395996\n",
      "loss validation: 1.121430516242981\n",
      "\t > Found a better model, 1.1519882678985596 -> 1.121430516242981\n",
      "\n",
      "Best model loss: 1.121430516242981\n",
      "Model Accuracy (Transfer Leraning): 62.6%\n"
     ]
    }
   ],
   "source": [
    "model_finetune = trained_model\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 5\n",
    "\n",
    "# Define our loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_finetune.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model_finetuned, train_loss, valid_loss = train_model(model_finetune, criterion, optimizer, trainloader_svhn, validloader_svhn, EPOCHS)\n",
    "\n",
    "# Test the model\n",
    "test_acc = get_accuracy(trained_model_finetuned, testloader_svhn)\n",
    "print(f\"Model Accuracy (Transfer Leraning): {test_acc*100}%\")\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 52/52 - Loss: 1.1462393999099731\n",
      "loss validation: 1.1520280838012695\n",
      "\t > Found a better model, 5 -> 1.1520280838012695\n",
      "Epoch 2, batch 52/52 - Loss: 1.271073579788208\n",
      "loss validation: 1.1316075325012207\n",
      "\t > Found a better model, 1.1520280838012695 -> 1.1316075325012207\n",
      "Epoch 3, batch 52/52 - Loss: 1.1436316967010498\n",
      "loss validation: 1.1628005504608154\n",
      "Epoch 4, batch 52/52 - Loss: 1.1388542652130127\n",
      "loss validation: 1.1367510557174683\n",
      "Epoch 5, batch 52/52 - Loss: 1.1298797130584717\n",
      "loss validation: 1.184921145439148\n",
      "\n",
      "Best model loss: 1.1316075325012207\n",
      "Model Accuracy (Feature Extraction): 61.9%\n"
     ]
    }
   ],
   "source": [
    "model_featext = trained_model\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 5\n",
    "\n",
    "# Freeze all layers except the last few layers\n",
    "for name, param in model_featext.named_parameters():\n",
    "    if \"fc2\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimixer\n",
    "optimizer = torch.optim.Adam(model_featext.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model_featexted, train_loss, valid_loss = train_model(model_featext, criterion, optimizer, trainloader_svhn, validloader_svhn, EPOCHS)\n",
    "\n",
    "# Test the model\n",
    "test_acc = get_accuracy(trained_model_featexted, testloader_svhn)\n",
    "print(f\"Model Accuracy (Feature Extraction): {test_acc*100}%\")\n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
